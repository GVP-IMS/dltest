{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get proper training data\n",
    "def get_xy(filename, max_frames, max_classes):\n",
    "\n",
    "    data = json.load(open(filename, 'r'))\n",
    "    xy_data = []\n",
    "    classes = {\"10\":0, \"00\":0, \"11\":0, \"01\":0}\n",
    "\n",
    "    for entry in data.items():\n",
    "        entry_content = entry[1]\n",
    "        entry_val = int(entry_content[\"valence\"])\n",
    "        entry_act = int(entry_content[\"activation\"])\n",
    "        entry_emotion = f\"{entry_val}{entry_act}\"\n",
    "        entry_frames = entry_content[\"features\"]\n",
    "\n",
    "        # Normalize frames\n",
    "        for frame in entry_frames:\n",
    "            for counter, number in enumerate(frame):\n",
    "                frame[counter] = float(round(number, 5))\n",
    "\n",
    "        # Equalize classes\n",
    "        for key in classes:\n",
    "            if entry_emotion == key:\n",
    "                if classes[key] < max_classes:\n",
    "                    classes[key] += 1\n",
    "                    xy_data.append([entry_frames,float(entry_val),float(entry_act)])\n",
    "\n",
    "    # Equalize frames\n",
    "    for entry in xy_data:\n",
    "        features_length = len(entry[0])\n",
    "        difference = features_length - max_frames\n",
    "        padding = [float(0) for counter in range(26)]\n",
    "        if difference < 0:\n",
    "            # Add the missing frames, each frame with 26 zeroes\n",
    "            for counter in range(abs(difference)):\n",
    "                entry[0].append(padding)\n",
    "        else:\n",
    "            for counter in range(difference):\n",
    "                entry[0].pop()\n",
    "\n",
    "    return xy_data\n",
    "\n",
    "class DL_Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        entry = self.data[index]\n",
    "        # Have to add another dimension because using CNN in Pytorch is dumb\n",
    "        frames = torch.tensor((entry[0])).unsqueeze(0)\n",
    "        val_label = torch.tensor((entry[1])).unsqueeze(0)\n",
    "        act_label = torch.tensor((entry[2])).unsqueeze(0)\n",
    "        return frames, val_label, act_label\n",
    "\n",
    "# Get proper testing data\n",
    "def get_sxy(filename, max_frames):\n",
    "\n",
    "    data = json.load(open(filename, 'r'))\n",
    "    xy_data = []\n",
    "\n",
    "    for entry in data.items():\n",
    "        entry_content = entry[1]\n",
    "        entry_frames = entry_content[\"features\"]\n",
    "\n",
    "        # Normalize frames\n",
    "        for frame in entry_frames:\n",
    "            for counter, number in enumerate(frame):\n",
    "                frame[counter] = float(round(number, 5))\n",
    "\n",
    "        xy_data.append([entry_frames])\n",
    "\n",
    "    # Equalize frames\n",
    "    for entry in xy_data:\n",
    "        features_length = len(entry[0])\n",
    "        difference = features_length - max_frames\n",
    "        padding = [float(0) for counter in range(26)]\n",
    "        if difference < 0:\n",
    "            # Add the missing frames, each frame with 26 zeroes\n",
    "            for counter in range(abs(difference)):\n",
    "                entry[0].append(padding)\n",
    "        else:\n",
    "            for counter in range(difference):\n",
    "                entry[0].pop()\n",
    "\n",
    "    return xy_data    \n",
    "\n",
    "class DL_SDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        entry = self.data[index]\n",
    "        frames = torch.tensor((entry[0])).unsqueeze(0)\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA HANDLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames = 200\n",
    "max_classes = 1000\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DL_Dataset(get_xy('train.json', max_frames, max_classes))\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = DL_Dataset(get_xy('dev.json', max_frames, max_classes))\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = DL_SDataset(get_sxy('ser_test_2.json', max_frames))\n",
    "sub_loader = torch.utils.data.DataLoader(dataset = sub_data, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.v1 = nn.Sequential(nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(), nn.BatchNorm2d(16), nn.MaxPool2d(2, 2))\n",
    "        self.v2 = nn.Sequential(nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(), nn.BatchNorm2d(16), nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.v3 = nn.Sequential(nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(), nn.BatchNorm2d(16), nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Flatten())\n",
    "        self.v4 = nn.Sequential(nn.Linear(2080, 1), nn.Sigmoid())\n",
    "        \n",
    "        self.a1 = nn.Sequential(nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(), nn.BatchNorm2d(16), nn.MaxPool2d(2, 2))\n",
    "        self.a2 = nn.Sequential(nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(), nn.BatchNorm2d(16), nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.a3 = nn.Sequential(nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(), nn.BatchNorm2d(16), nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Flatten())\n",
    "        self.a4 = nn.Sequential(nn.Linear(2080, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        vout = self.v1(x)\n",
    "        vout = self.v2(vout)\n",
    "        vout = self.v3(vout)\n",
    "        vout = self.v4(vout)\n",
    "\n",
    "        aout = self.a4(self.a3(self.a2(self.a1(x))))\n",
    "\n",
    "        return vout, aout\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = Model2().to(device)\n",
    "num_epochs = 5\n",
    "learning_rate = 0.0001\n",
    "loss_fn1 = nn.BCELoss()\n",
    "loss_fn2 = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    cc = 0\n",
    "    for frames, val, act in train_loader:\n",
    "        frames, val, act = frames.to(device), val.to(device), act.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        vout, aout = model(frames)\n",
    "        loss_val = loss_fn1(vout, val)\n",
    "        loss_act = loss_fn2(aout, act)\n",
    "        total_loss = loss_val + loss_act\n",
    "\n",
    "        cc += 1\n",
    "        if cc == 1:\n",
    "            #print(f\"{frames.size()} {val.size()} {act.size()} {vout.size()} {aout.size()}\")\n",
    "            pass\n",
    "\n",
    "        loss_val.backward()\n",
    "        loss_act.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss}, VL: {loss_val}, AL: {loss_act}')\n",
    "\n",
    "        testo = {\"v1\":0, \"v0\":0, \"a1\":0, \"a0\":0}\n",
    "        for i in range(len(vout)):\n",
    "            if vout[i] > 0.5:\n",
    "                testo[\"v1\"] += 1\n",
    "            else:\n",
    "                testo[\"v0\"] += 1\n",
    "        for i in range(len(aout)):\n",
    "            if aout[i] > 0.5:\n",
    "                testo[\"a1\"] += 1\n",
    "            else:\n",
    "                testo[\"a0\"] += 1                 \n",
    "        #print(testo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries = 2928 / vacc = 0.4959016393442623 / aacc = 0.5836748633879781 / vcor: 1452 / acor: 1709 / loss: 0.35127782821655273\n"
     ]
    }
   ],
   "source": [
    "vcor = 0\n",
    "acor = 0\n",
    "entries = 0 # 30 batches x 100 entries\n",
    "for frames, val, act in test_loader:\n",
    "    entries += val.size(0)\n",
    "\n",
    "    vout, aout = model(frames)\n",
    "\n",
    "    listy = [val, act, vout, aout]\n",
    "    listy2 = []\n",
    "    for i in listy:\n",
    "        listy3 = []\n",
    "        for y in i:\n",
    "            if y.item() >= 0.5:\n",
    "                listy3.append(1)\n",
    "            else:\n",
    "                listy3.append(0)\n",
    "        listy2.append(listy3)\n",
    "    \n",
    "    for i in range(len(val)):\n",
    "        if listy2[0][i] == listy2[2][i]:\n",
    "            vcor += 1\n",
    "        if listy2[1][i] == listy2[3][i]:\n",
    "            acor += 1\n",
    "\n",
    "    if entries == 100:\n",
    "        #!!!!\n",
    "        #print(act[0].item())\n",
    "        #print(aout[0].item())\n",
    "        #print(f\"{vout.size()} {val.size()} {act.size()} {vout.size()} {aout.size()}\")\n",
    "        pass\n",
    "\n",
    "#print(f\"entries = {entries} / vacc = {(vcor/entries)} / aacc = {(acor/entries)} / vcor: {vcor} / acor: {acor} / loss: {total_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list = []\n",
    "act_list = []\n",
    "for frames in sub_loader:\n",
    "    vout, aout = model(frames)\n",
    "    for i in vout:\n",
    "        for y in i:\n",
    "            if y.item() >= 0.5:\n",
    "                val_list.append(1)\n",
    "            else:\n",
    "                val_list.append(0)\n",
    "    for i in aout:\n",
    "        for y in i:\n",
    "            if y.item() >= 0.5:\n",
    "                act_list.append(1)\n",
    "            else:\n",
    "                act_list.append(0)\n",
    "\n",
    "final = {}\n",
    "for i in range(len(val_list)):\n",
    "    final[str(i)] = {\"valence\": val_list[i], \"activation\": act_list[i]}\n",
    "\n",
    "with open('submission.json', 'w') as f:\n",
    "    json.dump(final, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 5587, 0: 851}\n",
      "{1: 3273, 0: 3165}\n"
     ]
    }
   ],
   "source": [
    "testo1 = {1:0, 0:0}\n",
    "testo2 = {1:0, 0:0}\n",
    "for i in val_list:\n",
    "    if i == 0:\n",
    "        testo1[0] += 1\n",
    "    elif i == 1:\n",
    "        testo1[1] += 1\n",
    "    else:\n",
    "        print(i)\n",
    "for i in act_list:\n",
    "    if i == 0:\n",
    "        testo2[0] += 1\n",
    "    elif i == 1:\n",
    "        testo2[1] += 1\n",
    "    else:\n",
    "        print(i)\n",
    "print(testo1)\n",
    "print(testo2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
